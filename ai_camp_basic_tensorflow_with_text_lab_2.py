# -*- coding: utf-8 -*-
"""AI Camp - Basic TensorFlow with Text - Lab 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X0wLb18NhMdCFUlBoreAnItSsjRh7x_P

# Basic TensorFlow with Text - Lab 2

ตรวจสอบเวอร์ชันของ TensorFlow ที่ติดตั้งใน Google Colab (อาจจะไม่จำเป็นแล้ว ตอนนี้ Colab ได้อัพเกรด TensorFlow เป็นเวอร์ชัน 2 แล้ว)
"""

import tensorflow as tf
print(tf.__version__)

"""ตัวอย่างต่อไปนี้ เป็นการทำ Text Classification สำหรับการคัดแยกข้อความเป็นข้อความประชดประชันหรือไม่ โดยดัดแปลงมาจาก Laurence Moroney ใน deeplearning.ai

## ขั้นตอนที่ 1
ดาวน์โหลดข้อมูลตัวอย่างจากลิงค์ต่อไปนี้:

https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json

## ขั้นตอนที่ 2
เปิดไฟล์และโหลดข้อมูลที่เป็น json เก็บในตัวแปร datastore
"""

!wget --no-check-certificate \
  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \
  -O /tmp/sarcasm.json

import json

with open("/tmp/sarcasm.json", 'r') as f:
  datastore = json.load(f)

datastore

"""Labeled Data หรือข้อมูลมีเฉลย

## ขั้นตอนที่ 3
- นำคอลัมน์ headline ใน datastore มาใส่ในตัวแปร sentences
- นำคอลัมน์ is_sarcastic ใน datastore มาใส่ในตัวแปร labels
- นำคอลัมน์ article_link ใน datastore มาใส่ในตัวแปร urls
- แบ่งข้อมูล 20000 อันแรก (ทั้งส่วน sentences และ labels) มาเป็น training_sentences
- ข้อมูลที่เหลือ (6709 ประโยค) ใช้เป็น testing_sentences
"""

import numpy as np

sentences = []
labels = []
#urls = []
for item in datastore:
  sentences.append(item['headline'])
  labels.append(item['is_sarcastic'])
  #urls.append(item['article_link'])

training_size = 20000
training_sentences = sentences[0:training_size]
testing_sentences  = sentences[training_size:]
print(len(testing_sentences))

training_labels = np.array(labels[0:training_size])
testing_labels  = np.array(labels[training_size:])
print(len(testing_labels))

"""## Hyper-parameters for modeling

ต่อไปนี้เป็น Hyper-Parameters ที่สามารถปรับได้ ใช้ใน Deep Learning เช่น vocal_size หรือจำนวนคำศัพท์ เป็นต้น
"""

vocab_size = 5000
embedding_dim = 1024
max_length = 512
trunc_type = 'post'
padding_type = 'post'
oov_tok = "<OOV>"

"""## Modeling:

การสร้างโมเดลใน Deep Learning
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index

training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = pad_sequences(training_sequences,
                                maxlen=max_length,
                                padding=padding_type,
                                truncating=trunc_type)


testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,
                                maxlen=max_length,
                                padding=padding_type,
                                truncating=trunc_type)

model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(24, activation='relu'),
  tf.keras.layers.Dense(48, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""## Training:

ตัวอย่างโค้ดในเซลล์ก่อนหน้านี้เป็นการสร้างร่างแบบของโมเดลว่าจะมีเลเยอร์ต่างๆ อย่างไรบ้าง โดยที่ยังไม่มีการนำข้อมูลมาปรับ หรือเรียกว่า Training

สิ่งที่ต้องกำหนดใน Training คือ จำนวนรอบที่ต้องการรัน (Epoch) เป็นต้น ในกรณีตัวอย่าง ให้รัน 10 รอบ
"""

num_epochs = 10

history = model.fit(training_padded, training_labels, epochs=num_epochs,
                    validation_data=(testing_padded, testing_labels), verbose=2)

"""## Visualize the output:

ผลลัพธ์ของโมเดลแสดงดังกราฟด้านล่างนี้ โดยแสดงด้วยค่าวัด 2 ค่าคือ Accuracy กับ Loss

ในรูปด้านบน Accuracy ของข้อมูลที่ใช้เทรน Deep Learning หรือเส้นสีฟ้า มีค่าสูงขึ้นตาม Epoch ที่รันเพิ่มเติม ในขณะที่ Accuracy ของข้อมูลที่ใช้ทดสอบ (val_accuracy) หรือเส้นสีส้มมีค่าลดลง

ส่วนค่า Loss ของข้อมูลที่ใช้เทรน Deep Learning มีค่าลดลง ในขณะที่ Loss ของข้อมูลที่ใช้ทดสอบ (val_loss) เส้นสีส้มมีค่าเพิ่มขึ้น

ผลลัพธ์ทั้งสองนี้แสดงให้เห็นว่า ในกรณีนี้ การทำ Text Classification ทำได้ดีเฉพาะกับข้อมูลที่ระบบเคยเห็นมาแล้ว หรือผลลัพธ์ดีกับข้อมูลที่ใช้เทรน แต่สำหรับข้อมูลที่ระบบไม่เคยเห็น (ข้อมูลที่ไม่ได้ใช้เทรน) ระบบให้ผลลัพธ์ที่ไม่ดี พฤติกรรมแบบนี้อาจเรียกได้ว่า เกิด Overfitted
"""

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

"""## แบบฝึกหัด
ทดลองปรับ Hyperparameter ต่าง ๆ จนทำให้กราฟของ Accuracy สำหรับข้อมูลชุดเทรนและชุดทดสอบมีค่าสูงขึ้น ในขณะที่ Loss สำหรับข้อมูลชุดเทรนและชุดทดสอบมีค่าลดลง
"""